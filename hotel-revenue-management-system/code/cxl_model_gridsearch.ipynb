{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moral-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from model_cancellations import split_reservations\n",
    "from features import X1_cxl_cols, X2_cxl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_columns = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "novel-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_res = pd.read_pickle(\"pickle/h1_res.pick\")\n",
    "h2_res = pd.read_pickle(\"pickle/h2_res.pick\")\n",
    "h1_dbd = pd.read_pickle(\"pickle/h1_dbd.pick\")\n",
    "h2_dbd = pd.read_pickle(\"pickle/h2_dbd.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = split_reservations(h1_res, \"2017-08-01\", features=X1_cxl_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "willing-latest",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-84fcb85b3c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mregularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "def regularization(X, y):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=42)\n",
    "    \n",
    "    # standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # logistic regression\n",
    "    lr = LogisticRegression(max_iter=500)\n",
    "    lr_model = lr.fit(X_train_scaled, y_train)\n",
    "    lr_score = lr_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    # ridge\n",
    "    ridge = RidgeCV(cv=5)\n",
    "    ridge_model = ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_score = ridge_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    # lasso\n",
    "    lasso = LassoCV(cv=5)\n",
    "    lasso_model = lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_score = lasso_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    print('\\nSIMPLE LR Validation F-1 score was:', lr_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "    print('Test score: ', lr_model.score(X_test, y_test))\n",
    "#     for feature, coef in zip(X.columns, lr_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "    \n",
    "    print('\\nRIDGE Validation F-1 score was:', ridge_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "#     for feature, coef in zip(X.columns, ridge_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "        \n",
    "    print('\\nLASSO Validation F-1 score was:', lasso_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "#     for feature, coef in zip(X.columns, lasso_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "    \n",
    "\n",
    "regularization(X1, y1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-vietnam",
   "metadata": {},
   "source": [
    "## XGBoost (Hotel 1): Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    "                          max_depth=5\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [380, 390, 400, 410, 420, 475]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_xgb_fit_1 = grid_search_1.fit(X1_train, y1_train)\n",
    "print(\"The best parameters are: \\n\", grid_xgb_fit_1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-victory",
   "metadata": {},
   "source": [
    "## XGBoost (Hotel 2): Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "# further tuning params\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [440, 450, 475, 500, 550]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_xgb_fit_2 = grid_search_2.fit(X2_train, y2_train)\n",
    "print(\"The best parameters are: \\n\", grid_xgb_fit_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-florence",
   "metadata": {},
   "source": [
    "### First grid search results\n",
    "\n",
    "**H1 grid search setup**\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.0001,0.01],\n",
    "    'max_depth': range(2,8,2),\n",
    "    'n_estimators': [200, 300, 400]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "```\n",
    "\n",
    "**And the results**:\n",
    "```\n",
    "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400}\n",
    "CPU times: user 53.5 s, sys: 414 ms, total: 53.9 s\n",
    "Wall time: 1h 27min 17s\n",
    "```\n",
    "\n",
    "R^2 score: 0.830629056415377\n",
    "\n",
    "**H2 grid search setup was the same as H1 grid search setup (round 1)**\n",
    "\n",
    "**And the results**:\n",
    "\n",
    "```\n",
    "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400}\n",
    "CPU times: user 1min 37s, sys: 262 ms, total: 1min 38s\n",
    "Wall time: 1h 32min 4s\n",
    "```\n",
    "\n",
    "R^2 score: 0.8276818353712341\n",
    "\n",
    "### Second Round\n",
    "\n",
    "H1 setup & results\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "    'max_depth': range(5,6,7),\n",
    "    'n_estimators': [350, 400, 450]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}\n",
    "CPU times: user 40.9 s, sys: 325 ms, total: 41.2 s\n",
    "Wall time: 1h 37min 13s\n",
    "```\n",
    "\n",
    "R^2 score: 0.8514727908137794\n",
    "\n",
    "H2 results (setup the same as H1 round 2):\n",
    "\n",
    "```\n",
    "\n",
    "# SETUP SAME AS H1 ROUND 2\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 450}\n",
    "CPU times: user 1min 29s, sys: 236 ms, total: 1min 29s\n",
    "Wall time: 1h 41min 45s\n",
    "```\n",
    "R^2 score: 0.84904827933946\n",
    "\n",
    "### Round 3\n",
    "\n",
    "H1 setup different than H2 setup this time.\n",
    "\n",
    "H1 setup:\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    "                          max_depth=5\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [380, 390, 400, 410, 420, 475]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.11, 'n_estimators': 475}\n",
    "CPU times: user 49.1 s, sys: 437 ms, total: 49.5 s\n",
    "Wall time: 4h 8min 47s\n",
    "```\n",
    "R^2 score: 0.854218671992012\n",
    "\n",
    "And H2 setup:\n",
    "\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "# further tuning params\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [440, 450, 475, 500, 550]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.12, 'n_estimators': 550}\n",
    "CPU times: user 2min 8s, sys: 187 ms, total: 2min 8s\n",
    "Wall time: 5h 5min 44s\n",
    "```\n",
    "\n",
    "R^2 score: 0.8564225387621328"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-dairy",
   "metadata": {},
   "source": [
    "## XGB Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepted-dating",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_xgb_fit_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f093c8ac6564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_xgb1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_xgb_fit_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrid_xgb1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_xgb_fit_1' is not defined"
     ]
    }
   ],
   "source": [
    "grid_xgb1_score = grid_xgb_fit_1.score(X1_test, y1_test)\n",
    "grid_xgb1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb2_score = grid_xgb_fit_2.score(X2_test, y2_test)\n",
    "grid_xgb2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb_fit_1.to_pickle(\"pickle/h1_grid_result_1.pick\")\n",
    "grid_xgb_fit_2.to_pickle(\"pickle/h2_grid_result_1.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
