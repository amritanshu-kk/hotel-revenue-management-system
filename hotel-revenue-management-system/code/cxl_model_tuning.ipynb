{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moral-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_columns = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "novel-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_pickle(\"pickle/X1_cxl.pick\")\n",
    "y1 = pd.read_pickle(\"pickle/y1_cxl.pick\")\n",
    "\n",
    "X2 = pd.read_pickle(\"pickle/X2_cxl.pick\")\n",
    "y2 = pd.read_pickle(\"pickle/y2_cxl.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(X1, y1, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "willing-latest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMPLE LR Validation R^2 score was: 0.8300049925112332\n",
      "Test score:  0.2829505741387918\n",
      "\n",
      "RIDGE Validation R^2 score was: 0.3150907949370658\n",
      "\n",
      "LASSO Validation R^2 score was: 0.31497972457165135\n"
     ]
    }
   ],
   "source": [
    "def regularization(X, y):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=42)\n",
    "    \n",
    "    # standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # logistic regression\n",
    "    lr = LogisticRegression(max_iter=500)\n",
    "    lr_model = lr.fit(X_train_scaled, y_train)\n",
    "    lr_score = lr_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    # ridge\n",
    "    ridge = RidgeCV(cv=5)\n",
    "    ridge_model = ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_score = ridge_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    # lasso\n",
    "    lasso = LassoCV(cv=5)\n",
    "    lasso_model = lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_score = lasso_model.score(X_val_scaled, y_val)\n",
    "    \n",
    "    print('\\nSIMPLE LR Validation F-1 score was:', lr_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "    print('Test score: ', lr_model.score(X_test, y_test))\n",
    "#     for feature, coef in zip(X.columns, lr_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "    \n",
    "    print('\\nRIDGE Validation F-1 score was:', ridge_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "#     for feature, coef in zip(X.columns, ridge_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "        \n",
    "    print('\\nLASSO Validation F-1 score was:', lasso_score)\n",
    "#     print('Feature coefficient results: \\n')\n",
    "#     for feature, coef in zip(X.columns, lasso_model.coef_):\n",
    "#         print(feature, ':', f'{coef:.2f}') \n",
    "    \n",
    "\n",
    "regularization(X1, y1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-vietnam",
   "metadata": {},
   "source": [
    "## XGBoost (Hotel 1): Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serial-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    "                          max_depth=5\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [380, 390, 400, 410, 420, 475]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "original-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "The best parameters are: \n",
      " {'learning_rate': 0.11, 'n_estimators': 475}\n",
      "CPU times: user 49.1 s, sys: 437 ms, total: 49.5 s\n",
      "Wall time: 4h 8min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_xgb_fit_1 = grid_search_1.fit(X1_train, y1_train)\n",
    "print(\"The best parameters are: \\n\", grid_xgb_fit_1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-victory",
   "metadata": {},
   "source": [
    "## XGBoost (Hotel 2): Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chemical-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "# further tuning params\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [440, 450, 475, 500, 550]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "excess-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-breakfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "The best parameters are: \n",
      " {'learning_rate': 0.12, 'n_estimators': 550}\n",
      "CPU times: user 2min 8s, sys: 187 ms, total: 2min 8s\n",
      "Wall time: 5h 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_xgb_fit_2 = grid_search_2.fit(X2_train, y2_train)\n",
    "print(\"The best parameters are: \\n\", grid_xgb_fit_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-florence",
   "metadata": {},
   "source": [
    "### First grid search results\n",
    "\n",
    "**H1 grid search setup**\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.0001,0.01],\n",
    "    'max_depth': range(2,8,2),\n",
    "    'n_estimators': [200, 300, 400]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "```\n",
    "\n",
    "**And the results**:\n",
    "```\n",
    "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400}\n",
    "CPU times: user 53.5 s, sys: 414 ms, total: 53.9 s\n",
    "Wall time: 1h 27min 17s\n",
    "```\n",
    "\n",
    "F-1 score: 0.830629056415377\n",
    "\n",
    "**H2 grid search setup was the same as H1 grid search setup (round 1)**\n",
    "\n",
    "**And the results**:\n",
    "\n",
    "```\n",
    "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400}\n",
    "CPU times: user 1min 37s, sys: 262 ms, total: 1min 38s\n",
    "Wall time: 1h 32min 4s\n",
    "```\n",
    "\n",
    "F-1 score: 0.8276818353712341\n",
    "\n",
    "### Second Round\n",
    "\n",
    "H1 setup & results\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "    'max_depth': range(5,6,7),\n",
    "    'n_estimators': [350, 400, 450]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}\n",
    "CPU times: user 40.9 s, sys: 325 ms, total: 41.2 s\n",
    "Wall time: 1h 37min 13s\n",
    "```\n",
    "\n",
    "F-1 score: 0.8514727908137794\n",
    "\n",
    "H2 results (setup the same as H1 round 2):\n",
    "\n",
    "```\n",
    "\n",
    "# SETUP SAME AS H1 ROUND 2\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 450}\n",
    "CPU times: user 1min 29s, sys: 236 ms, total: 1min 29s\n",
    "Wall time: 1h 41min 45s\n",
    "```\n",
    "F-1 score: 0.84904827933946\n",
    "\n",
    "### Round 3\n",
    "\n",
    "H1 setup different than H2 setup this time.\n",
    "\n",
    "H1 setup:\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    "                          max_depth=5\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [380, 390, 400, 410, 420, 475]\n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.11, 'n_estimators': 475}\n",
    "CPU times: user 49.1 s, sys: 437 ms, total: 49.5 s\n",
    "Wall time: 4h 8min 47s\n",
    "```\n",
    "F-1 score: 0.854218671992012\n",
    "\n",
    "And H2 setup:\n",
    "\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=42,\n",
    ")\n",
    "\n",
    "# further tuning params\n",
    "params = {\n",
    "    'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    'n_estimators': [440, 450, 475, 500, 550]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = params,\n",
    "    n_jobs=-1,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2,random_state=42)\n",
    "\n",
    "# ---------RESULTS----------\n",
    "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
    "The best parameters are: \n",
    " {'learning_rate': 0.12, 'n_estimators': 550}\n",
    "CPU times: user 2min 8s, sys: 187 ms, total: 2min 8s\n",
    "Wall time: 5h 5min 44s\n",
    "```\n",
    "\n",
    "F-1 score: 0.8564225387621328"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-dairy",
   "metadata": {},
   "source": [
    "## XGB Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accepted-dating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854218671992012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_xgb1_score = grid_xgb_fit_1.score(X1_test, y1_test)\n",
    "grid_xgb1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prepared-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8564225387621328"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_xgb2_score = grid_xgb_fit_2.score(X2_test, y2_test)\n",
    "grid_xgb2_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
